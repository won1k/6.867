\relax 
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Logistic Regression}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.1}{Regularization and Weights}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.2}{$L_1$ vs. $L_2$ Regularization}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.3}{Hyperparameter Selection by Validation}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Effects of regularization on the weights (coefficients) of logistic regression model for various values of $\lambda $, and under $L_1$ vs. $L_2$ regularization.\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Decision boundaries for the training set for artificial dataset 1, for $L_1$ vs $L_2$ regularization.\relax }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Best-performing regularizers and $\lambda $ values for each artificial dataset (selected on validation), with classification accuracy on corresponding test set.\relax }}{2}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{Support Vector Machine}}{2}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.1}{Implementation Tests}}{2}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.2}{Comparisons on Artificial Datasets}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Simple dataset for testing linear SVM implementation (a, b), and (c) classification accuracy on test set of artificial datasets.\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Decision boundaries for the training set for artificial datasets 1 through 4, using linear SVM with $C = 1$.\relax }}{3}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.3}{Hyperparameters for Linear and Gaussian RBF Kernels}}{3}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3}{Support Vector Machine with Pegasos}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Margin sizes and number of support vectors for varying $C$ (for linear kernels) as well as $\gamma $ (for Gaussian kernels).\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces (a): Margin sizes with respect to regularization strengths $\lambda $. (b, c, d): Decision boundaries for the training set in artificial dataset 3, with varying regularization strengths.\relax }}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.1}{Dependence on $\lambda $}}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.2}{Kernelized Soft-SVM for Nonlinear Features}}{4}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{17.77782pt}
\newlabel{tocindent2}{29.38873pt}
\newlabel{tocindent3}{0pt}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.3}{Hyperparameter Dependence of Kernelized Soft-SVM}}{5}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4}{Handwritten Digit Recognition with MNIST}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces (a): Number of support vectors as $\gamma $ varies. (b, c, d): Decision boundaries for the training set in artificial dataset 3, with varying kernel bandwidths.\relax }}{5}}
