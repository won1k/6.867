\relax 
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Logistic Regression}}{1}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{Support Vector Machine}}{1}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3}{Support Vector Machine with Pegasos}}{1}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4}{Handwritten Digit Recognition with MNIST}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.1}{Initialization and Hyperparameters}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Squared distance to correct solution $u$ using the negative Gaussian function for various starting points, step sizes, and convergence criteria. The labels on the plots are the starting points.\relax }}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.2}{Comparison to Finite Differences}}{2}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{4.3}{Batch vs. Stochastic Gradient Descent on Least Squares}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Squared distance to correct solution $u$ using the negative Gaussian function for various starting points, step sizes, and convergence criteria. The labels on the plots are the starting points.\relax }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Squared distance to correct solution $u$ using the quadratic bowl function for various starting points, step sizes, and convergence criteria. The labels on the plots are the starting points.\relax }}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Squared error of finite difference gradient approximations compared to the exact gradients for differing step sizes.\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (A) shows the squared error for the two algorithms, while (B) and (C) show number of gradient evaluations for batch and stochastic gradient descent, respectively.\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5}{Linear Basis Function Regression}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Polynomial fits to the cosine model, where the green line is the true model and the red line is the polynomial fit of degree $M$.\relax }}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.1}{Polynomial Basis}}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.2}{SSE Loss}}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.3}{Gradient-Based Methods for SSE Minimization}}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{5.4}{Cosine Basis}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Squared error of batch GD and SGD solutions to exact SSE-minimizing solution for various initial weights, step sizes, and convergence thresholds.\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The true data-generating function (green) compared to the MLE solution using an order-8 cosine basis (red).\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Ridge regression estimated weights for the cosine function data, using a cosine basis of order $M$.\relax }}{5}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6}{Ridge Regression}}{5}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.1}{Polynomial Basis Example}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Ridge regression estimated weights using a polynomial basis of order $M$ and weight decay penalty of $\lambda $.\relax }}{6}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{6.2}{Model Selection Using Validation Set}}{6}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{7}{Sparsity and LASSO}}{6}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{17.77782pt}
\newlabel{tocindent2}{29.38873pt}
\newlabel{tocindent3}{0pt}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Estimated fits to the data using MLE, ridge regression, and LASSO and the associated SSE on the test data and $L_2$ distance to the true weights.\relax }}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Estimated weights with lowest validation error, for MLE, ridge, and LASSO regressions, compared to true weights.\relax }}{7}}
