\relax 
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Implement Gradient Descent}}{1}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.1}{Initialization and Hyperparameters}}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Squared distance to correct solution $u$ using the negative Gaussian function for various starting points, step sizes, and convergence criteria. The labels on the plots are the starting points.\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Squared distance to correct solution $u$ using the quadratic bowl function for various starting points, step sizes, and convergence criteria. The labels on the plots are the starting points.\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Squared error of finite difference gradient approximations compared to the exact gradients for differing step sizes.\relax }}{2}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.2}{Comparison to Finite Differences}}{2}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{1.3}{Batch vs. Stochastic Gradient Descent on Least Squares}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (A) shows the squared error for the two algorithms, while (B) and (C) show number of gradient evaluations for batch and stochastic gradient descent, respectively.\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Polynomial fits to the cosine model, where the green line is the true model and the red line is the polynomial fit of degree $M$.\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{Linear Basis Function Regression}}{3}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.1}{Polynomial Basis}}{3}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.2}{SSE Loss}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Squared error of batch GD and SGD solutions to exact SSE-minimizing solution for various initial weights, step sizes, and convergence thresholds.\relax }}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The true data-generating function (green) compared to the MLE solution using an order-8 cosine basis (red).\relax }}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.3}{Gradient-Based Methods for SSE Minimization}}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{2.4}{Cosine Basis}}{4}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3}{Ridge Regression}}{4}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.1}{Polynomial Basis Example}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ridge regression estimated weights for the cosine function data, using a cosine basis of order $M$.\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Ridge regression estimated weights using a polynomial basis of order $M$ and weight decay penalty of $\lambda $.\relax }}{5}}
\@writefile{toc}{\contentsline {subsection}{\tocsubsection {}{3.2}{Model Selection Using Validation Set}}{5}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4}{Sparsity and LASSO}}{5}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{17.77782pt}
\newlabel{tocindent2}{29.38873pt}
\newlabel{tocindent3}{0pt}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Estimated fits to the data using MLE, ridge regression, and LASSO and the associated SSE on the test data and $L_2$ distance to the true weights.\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Estimated weights with lowest validation error, for MLE, ridge, and LASSO regressions, compared to true weights.\relax }}{6}}
